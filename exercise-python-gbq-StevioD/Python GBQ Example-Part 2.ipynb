{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Google Big Query Example\n",
    "\n",
    "This notebook is a continuation of `Python GBQ Example.ipynb`. That file uploaded data to GBQ on tables we created manually. This version will create a couple of tables (based on the same schema) and fill them with data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"C:\\\\users\\\\jchan\\\\dropbox\\\\teaching\\\\\"\n",
    "service_file = 'UMT-MSBA-7b4265df0ca4.json' # this is your authentication information  \n",
    "gbq_proj_id = 'umt-msba'  # change this to your project_id\n",
    "gbq_dataset_id = 'wedge_example' # and change this to your data set ID\n",
    "\n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And create a client to talk to GBQ\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our `client` variable holds a connection to the project. This is client is similar to a file handle--it allows you to \"talk\" to the project.  \n",
    "\n",
    "---\n",
    "\n",
    "## Creating a Table\n",
    "\n",
    "Let's create a couple of tables based on our Wedge table schema. First, put in a name for your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_table = \"chandler_wedge_test\"\n",
    "\n",
    "table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,my_table])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test to see if that table exists and, if it doesn't, create it as an empty table. There's not natively a function to test for table existence in GBQ, so we'll write our own, which I found on StackOverflow [here](https://stackoverflow.com/questions/28731102/bigquery-check-if-table-already-exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbl_exists(client, table_ref):\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tbl_exists(client, table_full_name) :\n",
    "    table_ref = client.create_table(\n",
    "        table = table_full_name\n",
    "    )\n",
    "else :\n",
    "    table_ref = client.get_table(table_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our table is empty and doesn't even have a schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = client.get_table(table_ref)\n",
    "print(\"Table {} contains {} columns\".format(table_ref.table_id,len(table.schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.schema_update_options = [\n",
    "    bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION # This allows us to modify the table. \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell has a massive set of SchemaField additions. I built this programmatically, so that you can just copy and paste it when you need it. But it's worth reading it *and* comparing it to the `wedge_transaction_schema.json` to see if it makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell does the actual loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clean_transArchive_201307_201309_small.csv\", \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(\n",
    "        source_file,\n",
    "        table_ref,\n",
    "        location=\"US\",  # Must match the destination dataset location.\n",
    "        job_config=job_config,\n",
    "    )  # API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll see how it did. This kind of code, that doesn't do the actual work but helps you monitor what's going on, is invaluable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.result()  # Waits for table load to complete.\n",
    "print(\n",
    "    \"Loaded {} rows into {}:{}.\".format(\n",
    "        job. , 'wedge_example', table_ref.table_id\n",
    "    )\n",
    ")\n",
    "\n",
    "# Checks the updated length of the schema\n",
    "table = client.get_table(table)\n",
    "print(\"Table {} now contains {} columns.\".format(table_ref.table_id, len(table.schema)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that allowed us to load 10K rows from the small version. Let's clean it out and load a larger version.\n",
    "\n",
    "First, the clean out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text =\"\".join(['DELETE FROM `',table_full_name,'` WHERE 1=1'])\n",
    "# you have to have WHERE clause in a DELETE for GBQ\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "query_job = client.query(\n",
    "    query_text,\n",
    "    location=\"US\",\n",
    "    job_config=job_config,\n",
    ")  # API request - starts the query\n",
    "\n",
    "query_job.result()  # Waits for the query to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we've deleted the rows, let's once again fill the table up, this time with a slightly larger file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1 # Need to skip the header row here.\n",
    "\n",
    "with open(\"small_trans_file.csv\", \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(\n",
    "        source_file,\n",
    "        table_ref,\n",
    "        location=\"US\",  # Must match the destination dataset location.\n",
    "        job_config=job_config,\n",
    "    )  # API request\n",
    "    \n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\n",
    "    \"Loaded {} rows into {}:{}.\".format(\n",
    "        job.output_rows, 'wedge_example', table_ref.table_id\n",
    "    )\n",
    ")\n",
    "\n",
    "# Checks the updated length of the schema\n",
    "table = client.get_table(table)\n",
    "print(\"Table {} now contains {} columns.\".format(table_ref.table_id, len(table.schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's query to see how many owners and records we now have loaded in\n",
    "\n",
    "qry = \"\"\"SELECT\n",
    "              COUNT(DISTINCT card_no) as Owners,\n",
    "              COUNT(*) as Records\n",
    "         FROM  \n",
    "             `umt-msba.wedge_example.chandler_wedge_test`\"\"\"\n",
    "\n",
    "# And we execute queries with `client.query`\n",
    "query_job = client.query(\n",
    "    qry,\n",
    "    location=\"US\",\n",
    ")\n",
    "\n",
    "for row in query_job :\n",
    "    print(f\"We have {row[0]} owners and {row[1]} total records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, some tasks for you:\n",
    "\n",
    "1. Using the console (in your browser), determine how many days of data we have in our new table. \n",
    "1. Use Python code to create a _new_ table with the Wedge schema. \n",
    "1. Try to upload one of the files from `WedgeZipOfZips_Small.zip`. Invariably you're going to get some errors. See if you can figure out what's going wrong. \n",
    "1. Try to upload a different file that's formatted differently (different delimiter, different presence of headers, etc.) If that one throws errors, can you make sense of them? \n",
    "1. Using whatever mechanism you want (Python, Excel, find-and-replace), try to get one of those smaller transaction files loaded into GBQ. \n",
    "1. Your goal next goal: load the full version of this file into a new table in GBQ using Python. Bonus goal: load a couple in to their own tables.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
