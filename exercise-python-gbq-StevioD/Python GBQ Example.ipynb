{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Google Big Query Example\n",
    "\n",
    "This notebook explores your ability to connect a Python notebook to a Google Big Query instance. *NOTE*: Before you try to run this notebook, you need to _complete_ steps 1 and 2 from this [page](https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python). For step 2, \"Setting up Authentication\", I recommend following the console directions. Pay attention to where you save your JSON authentication file, since you'll need to tell Python how to find it. \n",
    "\n",
    "There's a second part of step 2, where you set an environment variable at the command line to your credentials. I was never able to make that work on my machine, so I'm using a direct path in my code below.That second step of that requires a bit of command line work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"/Users/chandler/Dropbox/Teaching/\"\n",
    "service_file = 'umt-msba-037daf11ee16.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'umt-msba' # change this to your poroject. \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our `client` variable holds a connection to the project. This is client is similar to a file handle--it allows you to \"talk\" to the project. Let's begin by writing a query and running it. \n",
    "\n",
    "---\n",
    "\n",
    "### Querying Data\n",
    "\n",
    "In this next section we'll query some data from our GBQ instance. In order to have something to play with, we'll get a very approximate measure of spend by `card_no` for a portion of the July 2013 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example query. This uses a trick that Python concatenates adjacent string literals for you.\n",
    "# Note, you'll need to update the project_id and dataset_id in the query below (and throughout)\n",
    "query = (\n",
    "    \"SELECT card_no, sum(total) as TotalSales \"\n",
    "    \"FROM `umt-msba.wedge_example.transactions_201307_small` \"\n",
    "    \"WHERE total > 0 AND trans_type = 'I' \"\n",
    "    \"GROUP BY card_no \"\n",
    "    \"ORDER BY TotalSales DESC \"\n",
    ")\n",
    "\n",
    "# And we execute queries with `client.query`\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    location=\"US\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've called `client.query` and assigned the results, we can iterate over the result set. In this case we'll print the first $n$ rows in a nice format. Note that `row`, the name we give to the items produced when we iterate over  `query_job` is a tuple of length two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card 3 spent 144,021.33 dollars.\n",
      "Card 12539 spent 15,281.17 dollars.\n",
      "Card 10499 spent 7,924.94 dollars.\n",
      "Card 14140 spent 7,152.03 dollars.\n",
      "Card 25016 spent 4,502.70 dollars.\n",
      "Card 20074 spent 4,264.39 dollars.\n",
      "Card 14987 spent 3,169.13 dollars.\n",
      "Card 19501 spent 1,941.07 dollars.\n",
      "Card 10504 spent 1,052.63 dollars.\n",
      "Card 13758 spent 1,034.45 dollars.\n",
      "Card 50056 spent 698.63 dollars.\n",
      "Card 21517 spent 691.69 dollars.\n",
      "Card 18389 spent 644.69 dollars.\n",
      "Card 14345 spent 631.40 dollars.\n",
      "Card 18876 spent 611.77 dollars.\n",
      "Card 18489 spent 610.16 dollars.\n",
      "Card 19750 spent 600.29 dollars.\n",
      "Card 15876 spent 588.82 dollars.\n",
      "Card 25334 spent 582.00 dollars.\n",
      "Card 15967 spent 570.89 dollars.\n",
      "Card 13341 spent 558.53 dollars.\n",
      "Card 25181 spent 547.85 dollars.\n",
      "Card 20192 spent 547.45 dollars.\n",
      "Card 10714 spent 538.93 dollars.\n",
      "Card 20197 spent 503.11 dollars.\n",
      "Card 50021 spent 501.24 dollars.\n"
     ]
    }
   ],
   "source": [
    "cards = []\n",
    "\n",
    "for idx, row in enumerate(query_job) :\n",
    "    card, sales = row\n",
    "    print(\"Card {:.0f} spent {:,.2f} dollars.\".format(card,sales))\n",
    "    cards.append(card)\n",
    "    if idx == 25 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Uploading Data\n",
    "\n",
    "To add data to the dataset, we'll need to create a table and put data in it. We'll do a simple example here to illustrate the concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Manually\n",
    "\n",
    "This is the easist way, but it only works for small files. Go to the console, and click on your project (the one that you replaced `umt-msba` with in the above) and then on your dataset (like `wedge_example`). You'll see a blue \"+\" sign (in the new UI). Click on that and it will open a \"create table\" prompt. \n",
    "\n",
    "Once that's open, here are the options to choose as you go down: \n",
    "\n",
    "1. Create from \"Upload\"\n",
    "1. Select the file `department_lookup.csv` that's in the same folder as this notebook.\n",
    "1. Give your table a unique name. I'm using `chandler_test1` here.  \n",
    "1. Select \"Auto detect\" under \"Schema\"\n",
    "\n",
    "You don't need to worry about the other fields and checkboxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have data in the file that you can see via the console. Let's test it here.\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test1` \" # change to *your*` project ID, data set ID, and table name! \n",
    ")\n",
    "\n",
    "results = client.query(query)\n",
    "\n",
    "print(\"Let's print it ugly.\")\n",
    "print(list(results))\n",
    "\n",
    "print(\"\\nAnd now let's print it pretty.\\n\")\n",
    "\n",
    "for dept_id, dept_name in results :\n",
    "    print(f\"{dept_name} has id {dept_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Table Creation, Programmatic Upload\n",
    "\n",
    "Now we'll do something similar. We'll create the table manually but upload the data programmatically.  \n",
    "\n",
    "So go through similar steps to create the table via the console. Open up the \"create table\" prompt. \n",
    "\n",
    "Once that's open, here are the options to choose as you go down: \n",
    "\n",
    "1. Create from \"Empty Table\"\n",
    "1. Give your table a unique name. I'm using `chandler_test2`. \n",
    "1. Add two fields. One called \"department\" that will be an integer and one called \"dept_name\" that will be a string.\n",
    "\n",
    "You don't need to worry about the other fields and checkboxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by querying the data and seeing that it's empty.\n",
    "\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test2` \" # change to *your*` project ID, data set ID, and table name!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now run the query\n",
    "results = client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(results)) # Should be an empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put some data in the table. This is a bit messy *and* only works for small data sets, so I'll show you how this is done, but we won't probably use this technique in the future. This code is taken directly from the GBQ [docs on uploading data](https://cloud.google.com/bigquery/docs/loading-data-local#limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(\"wedge_example\") # set up references to the dataset and table.\n",
    "table_ref = dataset_ref.table(\"chandler_test2\") # use your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to establish a job that will run this import for us. A clunky step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig() \n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "input_file = \"department_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "    \n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_ref, table_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's query this table and see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have data in the file that you can see via the console. Let's test it here.\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test2` \" # change to *your*` project ID, data set ID, and table name! \n",
    ")\n",
    "\n",
    "results = client.query(query)\n",
    "print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "When you're working on the Wedge project, the table schema is *much* more complicated. If you've reached this point in the notebook and have some extra time, see if you can download the table schema from one of our Wedge tables.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
